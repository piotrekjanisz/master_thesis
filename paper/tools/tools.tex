
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
\chapter{Tools}


\graphicspath{{tools/figures/}}

% ----------------------------------------------------------------------
%: ----------------------- introduction content ----------------------- 
% ----------------------------------------------------------------------

This chapter will present brief introduction of tools used in building visualization system. This is no intended to be a tutorial as those information can be found on the Internet. 

\section{Opengl}
OpenGL is an API specification for interacting with graphic hardware in order to produce 2D and 3D graphic. 
As a specification it is not bound to any programming language, operating system or hardware. 
OpenGL bindings exists for many languages including C/C++, Java, Python, Ruby and C$\sharp$ (see~\cite{OglBindings}). 

Since introduction of version 3.2 specification has been divided into core profile and compatibility profile. Core profile is smaller and contains only modern style API introducted with version 3.0. Compatibility profile implements all legacy functionality prior to OpenGL 3.0 including fixed pipeline. 

Fixed function pipeline is built-in to hardware and there is no control over how operations are performed on GPU. That means operations like vertex transformation, algorithms for shading surfaces are already implemented and can't be changed. The behavior may only be changed by choosing one of predefined methods or changing available parameters. Pros are that it's faster and less error prone to build an application. Programmers only have to choose existing technique and specify geometry primitives. On the other hand when new algorithm is invented there is no possibility to use it. There are also no possibilities to optimize existing techniques for the particular application. 
That is why programmable pipeline was introduced in modern GPUs. Rendering stages can be customized by writing programs called shaders. At first those programs were very specific and allowed little customization. Over time more and more capabilities were added to shaders and now their usage goes beyond graphic rendering \cite[Part~VI]{GPUGems3}. Currently programmers have full control over how primitives are rendered on the screen.

Apart from core and legacy profiles there are extensions. This is a mechanism for including vendor specific functions implemented only on some GPUs that can make use of some new hardware features. 

OpenGL is widely available on different hardware from graphic cluster, through desktop computers, game consoles and portable devices. For the last group dedicated standard has been created - OpenGL ES. It a subset of regular specification designed to run efficiently on devices with limited hardware resources. Most frequently used and most useful functionality of OpenGL 2.1 wast included in OpenGL ES. That was desired as larger API requires more complex and larger driver to support it. It's worth mentioning that OpenGL ES is the only officially supported 3D API for portable devices running under Android and iOS operating systems. 

\figuremacro{opengl_pipeline}{OpenGL 3.3 pipeline}{taken from \cite[chapter~12]{OpenGLSuperbible}}

Figure~\ref{opengl_pipeline} shows diagram of OpenGL 3.3 pipeline. Three stages are fully programmable - vertex shader, geometry shader and fragment shader. 

\subsection{Vertex Shader}
Input to the vertex shader program is a single vertex (vertices are processed in parallel) with optional attributes (which can vary between vertices - i.e.  normal vector, color, texture coordinate) and uniforms (which have the same value for all vertices - i.e. projection matrix, model matrix). This stage typically transforms vertices from model space to view space and applies perspective projection. Other typical applications are calculating perspective lighting and texturing calculations. More advanced operations includes procedural animation by modifying the position of the vertex. This can be used to animate water surfaces (like pond or oceans), clothes or skin. On modern GPUs the vertex shader has also access to texture data - especially useful when using textures as data-structures in physical simulations (see \cite[pages~412-419]{OpenGLSuperbible}).

\subsection{Geometry Shader}
This stage takes entire primitives (triangles, lines, points) as an input. The geometry shader can produce new primitives, discard existing or change their type. Thus it is capable of changing amount of data in pipeline, which is unique in contrast to vertex and fragment shaders (fragment shaders can only discard fragments). There are many examples of using geometry shader - from simple as rendering six faces of a cube map \cite{Gregory2009}, drawing vertices normals \cite[pages~434-437]{OpenGLSuperbible} to more advanced as shadow volume extrusion \cite[section~10.3.3.1]{Gregory2009}, isosurface extraction \cite{3DCourseSIGGraph2007} and dynamic tessellation. 

\subsection{Fragment Shader}
This stage operates on fragments. One fragment is usually one pixel on the screen, except cases when multisampling or supersamplings are used - then one several fragments may contribute to one pixel. The fragment shader job is to process pixels by computing per pixel lighting or applying textures. Input to this stage are per-fragment attributes passed from vertex or geometry shaders, uniform attributes which are passed before rendering and which have the same value for all fragments and textures. As vertex and geometry shaders operates on vertices, per-fragment attributes passed to fragment shader have to be interpolated. There are two types of interpolation flat and smooth. Flat means that all fragments in the primitive has the same value of attribute. Smooth means that values are interpolated between edges of primitive. 

As it operates at pixel level it is perfect for post processing effects (the one added on rendered scene) like blurring, bloom, different sort of filtering. In my project I make heavy use of fragment shaders. 

\subsection{Perspective projection and homogeneous space}
Perspective projection mimics the kind of image produced by a typical camera by providing effect known as perspective foreshortening. With this effect the farther objects lies from camera the smaller it appears on the screen. 
Perspective projection transforms points form view space into homogeneous clip space. In OpenGL view space uses right handed coordinate system (figure~\ref{perspective_projection}) and homogeneous space uses left handed coordinate system (figure~\ref{homogenous_space}). 
The region that is visible from camera is called the view volume or view frustum (figure~\ref{perspective_projection}). It is defined by six  planes - near plane, far plane and four side planes. Only objects inside view volume appears on the screen after rendering. Objects are projected on near plane which part that lies inside four side planes is a virtual screen.
Virtual screen left, right, bottom and top edges lies at $x=l, x=r, y=b, and y=t$. It's z coordinate is equal to -n. Those parameters are used to computed projection matrix $M_{V \to H }$ (equation~\ref{eq:projection_matrix}) ($V \to H$ means transformation from view space into homogeneous space):
\begin{equation}
\label{eq:projection_matrix}
M_{V \to H } = 
\begin{bmatrix}
\frac{2n}{w} & 0            & 0                & 0                \\ 
0            & \frac{2h}{n} &                  & 0                 \\ 
0            & 0            & -\frac{f+n}{f-n} & -\frac{2fn}{f-n} \\ 
0            & 0            & -1               & 0
\end{bmatrix}
\end{equation}
where $w=r-l$ and $h=t-b$.

\myFigureMacroW{perspective_projection}{A perspective view volume (frustum)}{1.0}

\myFigureMacroW{homogenous_space}{The canonical view volume in homogeneous clip space}{0.7}



\subsection{Depth buffer}
Dept buffer is used to determine which fragments occlude other fragments - thus which should be drawn on the screen. This is done by comparing value of z component of fragment from homogeneous space. 
After multiplying point from view space $p_V = [x, y, z, 1]$  by $M_{V \to H}$ we get point $p_H$:
\begin{equation}
\label{eg:projection_transformation}
p_H =
\begin{bmatrix}
\frac{2n}{w} & 0            & 0                & 0                \\ 
0            & \frac{2h}{n} &                  & 0                 \\ 
0            & 0            & -\frac{f+n}{f-n} & -\frac{2fn}{f-n} \\ 
0            & 0            & -1               & 0
\end{bmatrix}
*[x, y, z, 1]^T
\end{equation}
thus we get:
\begin{equation}
\label{eg:after_perspective_projection}
p_H = [\frac{2n}{w}x, \frac{2n}{h}y, -\frac{f+n}{f-n}z - \frac{2fn}{f-n}, -z]
\end{equation}
To transform $p_H$ into 3D space we need to divide x, y and z components by $\omega$ which is equal to $-z$. That gives us:
\begin{equation}
\label{eg:after_perspective_projection}
p_H = [-\frac{2nx}{wz},-\frac{2ny}{hz}, \frac{f+n}{f-n} + \frac{2fn}{z(f-n)}]
\end{equation}
After transformation z component of points inside view frustum varies from [-1, 1]. This has to be transform to [0, 1], thus value of depth buffer d(z) is following:
\begin{equation}
\label{eq:depth_value}
d(z) = \frac{\frac{f+n}{f-n} + \frac{2fn}{z(f-n)} + 1}{2}
\end{equation}
Equation~\ref{eq:depth_value} shows that value of depth buffer is a nonlinear function of z component of the point. Figure~\ref{z_buffer_dist} shows how this value changes with changing near plane distance from the viewer. It can also be seen that precision if not evenly distributed and it decreases with the distance from near plane. 

\myFigureMacroW{z_buffer_dist}{Value of z buffer as a function of distance from near plane for different values of n. $f-n$ is kept constant at 100}{1.0}

\subsection{Framebuffer Object}
Normally after performing drawing operations output is directed to framebuffers provided by windowing system. From those buffers data can be displayed on the computer screen. However, sometimes it's not desired for data to be displayed on the screen but to be used as an input to the rendering pipeline again. This technique is called off screen rendering. It can be achieved by copying data from default framebuffers to textures and than used as an input to the pipeline again but this requires additional copy operation that should be avoided. Framebuffer objects are mechanism for dealing with situation when data shouldn't be send directly to the display. Frame buffer objects are containers that can hold other object that have memory storage and can be rendered to \cite[chapter~8]{OpenGLSuperbible}. Two kinds of objects that can be rendered to are renderbuffer objects and textures. Rendering to textures is particularly useful as data doesn't have to be copied to be used as an input to the pipeline. This allows to efficiently implementing rendering techniques that requires multiple phases.
Unlike default framebuffer object, the one created by user is not limited in size to the size of rendering window. This can be used to speedup some computionally expensive processing by rendering with lower resolution into texture and than applying this texture to the original window with linear filtering. 

Details about framebuffer objects, their creation and attaching storage to then can be found in \cite[chapter~8]{OpenGLSuperbible} and \cite[chapter~10]{RedBook}.

\section{boost}
Boost is a set of libraries extending capabilities of C++ language. Libraries provides classes and functions for most common task and algorithms, like reqular expressions, hash maps, threading. They are also cross platform. 

The advantage of boost libraries is that they are designed for maximum flexibility and speed. They make heavy use of C++ template programming to be as general as possible. The other advantage of boost libraries is that they often become included in C++ standard - like regular expressions (FIXME). 

On the other hand relying on templates makes compiling times longer. Template classes and functions must be defined entirely in header files what makes it impossible to compile them into object files. Thus whenever file has to be recompiled template classes have to be generated over again.

Boost libraries used in my project are boost regex and boost threads. The first one is used to process configuration files. Boost trheads library is used to parallelize isosurface extraction algorithm presented in (TODO reference).

TODO reference to documentation or main page

\section{PhysX}

PhysX is a framework for performing physical simulations in real time. It's designed to be used in computer games and is optimized for performance. It provides most necessary models required in games: rigid bodies, soft bodies, cloths, fluids and joints. What is more it allows simulations to be performed on GPU which is much faster especially when large amounts of object acts in a simulations.

The main drawback is that hardware acceleration can only be performed on Nvidia GPUs. 

PhysX works in asynchronous way - that means simulation step is computed either on GPU or CPU thread and other CPU cores can perform some other work in the meantime - for example render scene using data from previous step. The asynchronous workflow of PhysX is presented in figure~\ref{physx_workflow}.

\myFigureMacroW{physx_workflow}{Diagram of PhysX SDK workflow}{0.7}

My project uses two PhysX features - rigid body and water simulations and thus those will be described in more details. 
\subsection{Rigid bodies}


\subsection{Fluids}
PhysX provides three methods for simulating fluids SPH, simple method and mixed method. First one was described in chapter~\ref{chap:sph}. Second one does not take interparticle forces into concideration thus simplifies computations. This simplified method is good for simulating phenomenas such as smoke or fog, but for realistic water SPH must be used. Third one - mixed method - according to documentation \cite{PhysXDoc} alternates between SPH and simple mode, providing more performance than SPH but still maintaining some dense characteristics.

Implementation details of PhysX's SPH method are covered in \cite{Harris08}. It solves conservation of momentum equation~\ref{eq:momentum_conservation} to move particles. External force includes only gravity force. Symmetrization of pressure and viscosity forces is done as in equations \ref{eq:sph_pressure_force_muller} and \ref{eq:sph_viscosity_force_sym}. TODO surface tension.

TODO collisions with other objecs, viscosity, surface tension. List of parameters.

PhysX Documentation - \cite{PhysXDoc}
On implementation of SPH in PhysX \cite{Harris08}.

TODO more details about fluids in PhysX - simulation methods, parameters, compare with SPH chapter, emiters, drains, limitations (64K particles per fluid)

TODO $ftp://download.nvidia.com/developer/cuda/seminar/TDCI_PhysX.pdf$

TODO prezentacja
% ----------------------------------------------------------------------



