
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------
\chapter{Tools}


\graphicspath{{tools/figures/}}

% ----------------------------------------------------------------------
%: ----------------------- introduction content ----------------------- 
% ----------------------------------------------------------------------

This chapter will present brief introduction of tools used in building visualization system. This is no intended to be a tutorial as those information can be found on the Internet. 

\section{Opengl}
OpenGL is an API specification for interacting with graphic hardware in order to produce 2D and 3D graphic. 
As a specification it is not bound to any programming language, operating system or hardware. 
OpenGL bindings exists for many languages including C/C++, Java, Python, Ruby and C$\sharp$ (see~\cite{OglBindings}). 

Since introduction of version 3.2 specification has been divided into core profile and compatibility profile. Core profile is smaller and contains only modern style API introducted with version 3.0. Compatibility profile implements all legacy functionality prior to OpenGL 3.0 including fixed pipeline. 

Fixed function pipeline is built-in to hardware and there is no control over how operations are performed on GPU. That means operations like vertex transformation, algorithms for shading surfaces are already implemented and can't be changed. The behavior may only be changed by choosing one of predefined methods or changing available parameters. Pros are that it's faster and less error prone to build an application. Programmers only have to choose existing technique and specify geometry primitives. On the other hand when new algorithm is invented there is no possibility to use it. There are also no possibilities to optimize existing techniques for the particular application. 
That is why programmable pipeline was introduced in modern GPUs. Rendering stages can be customized by writing programs called shaders. At first those programs were very specific and allowed little customization. Over time more and more capabilities were added to shaders and now their usage goes beyond graphic rendering \cite[Part~VI]{GPUGems3}. Currently programmers have full control over how primitives are rendered on the screen.

Apart from core and legacy profiles there are extensions. This is a mechanism for including vendor specific functions implemented only on some GPUs that can make use of some new hardware features. 

OpenGL is widely available on different hardware from graphic cluster, through desktop computers, game consoles and portable devices. For the last group dedicated standard has been created - OpenGL ES. It a subset of regular specification designed to run efficiently on devices with limited hardware resources. Most frequently used and most useful functionality of OpenGL 2.1 wast included in OpenGL ES. That was desired as larger API requires more complex and larger driver to support it. It's worth mentioning that OpenGL ES is the only officially supported 3D API for portable devices running under Android and iOS operating systems. 

\figuremacro{opengl_pipeline}{OpenGL 3.3 pipeline}{taken from \cite[chapter~12]{OpenGLSuperbible}}

Figure~\ref{opengl_pipeline} shows diagram of OpenGL 3.3 pipeline. Three stages are fully programmable - vertex shader, geometry shader and fragment shader. 

\subsection{Vertex Shader}
Input to the vertex shader program is a single vertex (vertices are processed in parallel) with optional attributes (which can vary between vertices - i.e.  normal vector, color, texture coordinate) and uniforms (which have the same value for all vertices - i.e. projection matrix, model matrix). This stage typically transforms vertices from model space to view space and applies perspective projection. Other typical applications are calculating perspective lighting and texturing calculations. More advanced operations includes procedural animation by modifying the position of the vertex. This can be used to animate water surfaces (like pond or oceans), clothes or skin. On modern GPUs the vertex shader has also access to texture data - especially useful when using textures as data-structures in physical simulations (see \cite[pages~412-419]{OpenGLSuperbible}).

\subsection{Geometry Shader}
This stage takes entire primitives (triangles, lines, points) as an input. The geometry shader can produce new primitives, discard existing or change their type. Thus it is capable of changing amount of data in pipeline, which is unique in contrast to vertex and fragment shaders (fragment shaders can only discard fragments). There are many examples of using geometry shader - from simple as rendering six faces of a cube map \cite{Gregory2009}, drawing vertices normals \cite[pages~434-437]{OpenGLSuperbible} to more advanced as shadow volume extrusion \cite[section~10.3.3.1]{Gregory2009}, isosurface extraction \cite{3DCourseSIGGraph2007} and dynamic tessellation. 

\subsection{Fragment Shader}
This stage operates on fragments. One fragment is usually one pixel on the screen, except cases when multisampling or supersamplings are used - then one several fragments may contribute to one pixel. The fragment shader job is to process pixels by computing per pixel lighting or applying textures. Input to this stage are per-fragment attributes passed from vertex or geometry shaders, uniform attributes which are passed before rendering and which have the same value for all fragments and textures. As vertex and geometry shaders operates on vertices, per-fragment attributes passed to fragment shader have to be interpolated. There are two types of interpolation flat and smooth. Flat means that all fragments in the primitive has the same value of attribute. Smooth means that values are interpolated between edges of primitive. 

As it operates at pixel level it is perfect for post processing effects (the one added on rendered scene) like blurring, bloom, different sort of filtering. In my project I make heavy use of fragment shaders. 

\subsection{Perspective projection and homogeneous space}
Perspective projection mimics the kind of image produced by a typical camera by providing effect known as perspective foreshortening. With this effect the farther objects lies from camera the smaller it appears on the screen. 
Perspective projection transforms points form view space into homogeneous clip space. In OpenGL view space uses right handed coordinate system (figure~\ref{perspective_projection}) and homogeneous space uses left handed coordinate system (figure~\ref{homogenous_space}). 
The region that is visible from camera is called the view volume or view frustum (figure~\ref{perspective_projection}). It is defined by six  planes - near plane, far plane and four side planes. Only objects inside view volume appears on the screen after rendering. Objects are projected on near plane which part that lies inside four side planes is a virtual screen.
Virtual screen left, right, bottom and top edges lies at $x=l, x=r, y=b, and y=t$. It's z coordinate is equal to -n. Those parameters are used to computed projection matrix $M_{V \to H }$ (equation~\ref{eq:projection_matrix}) ($V \to H$ means transformation from view space into homogeneous space):
\begin{equation}
\label{eq:projection_matrix}
M_{V \to H } = 
\begin{bmatrix}
\frac{2n}{w} & 0            & 0                & 0                \\ 
0            & \frac{2h}{n} &                  & 0                 \\ 
0            & 0            & -\frac{f+n}{f-n} & -\frac{2fn}{f-n} \\ 
0            & 0            & -1               & 0
\end{bmatrix}
\end{equation}
where $w=r-l$ and $h=t-b$.

\myFigureMacroW{perspective_projection}{A perspective view volume (frustum)}{1.0}

\myFigureMacroW{homogenous_space}{The canonical view volume in homogeneous clip space}{0.7}



\subsection{Depth buffer}
Dept buffer (also called z-buffer) is used to determine which fragments occlude other fragments - thus which should be drawn on the screen. This is done by comparing value of z component of fragment from homogeneous space. 
After multiplying point from view space $p_V = [x, y, z, 1]$  by $M_{V \to H}$ we get point $p_H$:
\begin{equation}
\label{eg:projection_transformation}
p_H =
\begin{bmatrix}
\frac{2n}{w} & 0            & 0                & 0                \\ 
0            & \frac{2h}{n} &                  & 0                 \\ 
0            & 0            & -\frac{f+n}{f-n} & -\frac{2fn}{f-n} \\ 
0            & 0            & -1               & 0
\end{bmatrix}
*[x, y, z, 1]^T
\end{equation}
thus we get:
\begin{equation}
\label{eg:after_perspective_projection}
p_H = [\frac{2n}{w}x, \frac{2n}{h}y, -\frac{f+n}{f-n}z - \frac{2fn}{f-n}, -z]
\end{equation}
To transform $p_H$ into 3D space we need to divide x, y and z components by $\omega$ which is equal to $-z$. That gives us:
\begin{equation}
\label{eg:after_perspective_projection}
p_H = [-\frac{2nx}{wz},-\frac{2ny}{hz}, \frac{f+n}{f-n} + \frac{2fn}{z(f-n)}]
\end{equation}
After transformation z component of points inside view frustum varies from [-1, 1]. This has to be transform to [0, 1], thus value of depth buffer d(z) is following:
\begin{equation}
\label{eq:depth_value}
d(z) = \frac{\frac{f+n}{f-n} + \frac{2fn}{z(f-n)} + 1}{2}
\end{equation}
Equation~\ref{eq:depth_value} shows that value of depth buffer is a nonlinear function of z component of the point. Figure~\ref{z_buffer_dist} shows how this value changes with changing near plane distance from the viewer. It can also be seen that precision if not evenly distributed and it decreases with the distance from near plane. This can lead to so called z-fighting which occurs when two objects have close z-buffer values. When the objects are close to viewer the z-values are distinguishable value however when objects get further values can get equal due to lack of precision (see~\ref{z_fighting}).

Z-fighting can be avoided when reasonable values are chosen for far and near planes. However there are cases when depth buffer when linear values are needed (see section~\ref{sec:gaussiansmoothing} TODO curvature flow reference). Alternative technique to Z-buffering is W-buffering which offers better distribution of depth values \cite{Gregory2009}. Some hardware supports W-buffering but most of it doesn't and it doesn't seems to be supported in future. A workaround to this problem was presented in \cite{Dunlop2006}. It customizes projection transformation in vertex shader in way that depth buffer values have linear distribution at the end. 

Linear depth value can also be obtained from equation~\ref{eg:after_perspective_projection} by dividing $\omega$ component of vector $p_H$ by $f$:
\begin{equation}
\label{eq:linear_depth}
d_{lin}(z) = \frac{-z}{f}
\end{equation}
since $-z$ is in range [n, f] value of $d_{lin}(z)$ will be in range $[\frac{n}{f}, 1]$. 

\figuremacroW{z_fighting}{Z-fighting}{two polygons rendered - on the left when z-fighting occurs, on the right the same polygons rendered correctly.}{1.0}
\myFigureMacroW{z_buffer_dist}{Value of z buffer as a function of distance from near plane for different values of n. $f-n$ is kept constant at 100}{1.0}

\subsection{Framebuffer Object}
Normally after performing drawing operations output is directed to framebuffers provided by windowing system. From those buffers data can be displayed on the computer screen. However, sometimes it's not desired for data to be displayed on the screen but to be used as an input to the rendering pipeline again. This technique is called off screen rendering. It can be achieved by copying data from default framebuffers to textures and than used as an input to the pipeline again but this requires additional copy operation that should be avoided. Framebuffer objects are mechanism for dealing with situation when data shouldn't be send directly to the display. Frame buffer objects are containers that can hold other object that have memory storage and can be rendered to \cite[chapter~8]{OpenGLSuperbible}. Two kinds of objects that can be rendered to are renderbuffer objects and textures. Rendering to textures is particularly useful as data doesn't have to be copied to be used as an input to the pipeline. This allows to efficiently implementing rendering techniques that requires multiple phases.
Unlike default framebuffer object, the one created by user is not limited in size to the size of rendering window. This can be used to speedup some computionally expensive processing by rendering with lower resolution into texture and than applying this texture to the original window with linear filtering. 

Details about framebuffer objects, their creation and attaching storage to then can be found in \cite[chapter~8]{OpenGLSuperbible} and \cite[chapter~10]{RedBook}.

\section{Boost library}
Boost is a set of libraries extending capabilities of C++ language. Libraries provides classes and functions for most common task and algorithms, like reqular expressions, hash maps, threading. They are also cross platform. 

The advantage of boost libraries is that they are designed for maximum flexibility and speed. They make heavy use of C++ template programming to be as general as possible. The other advantage of boost libraries is that they often become included in C++ standard library - like regular expressions (see~\cite{CppRef}) . 

On the other hand relying on templates makes compiling times longer. Template classes and functions must be defined entirely in header files what makes it impossible to compile them into object files. Thus whenever file has to be recompiled template classes have to be generated over again.

Boost libraries used in my project are \textit{boost regex} and \textit{boost threads}. The first one is used to process configuration files. \textit{Boost theads} library is used to parallelize isosurface extraction algorithm presented in section~\ref{sec:isosurfaceextraction}.

\section{PhysX}

PhysX is a framework for performing physical simulations in real time. It's designed to be used in computer games and is optimized for performance. It provides most necessary models required in games: rigid bodies, soft bodies, cloths, fluids and joints. What is more it allows simulations to be performed on GPU which is much faster especially when large amounts of object acts in a simulations.

The main drawback is that hardware acceleration can only be performed on Nvidia GPUs. 

PhysX works in asynchronous way - that means simulation step is computed either on GPU or CPU thread and other CPU cores can perform some other work in the meantime - for example render scene using data from previous step. The asynchronous workflow of PhysX is presented in figure~\ref{physx_workflow}.

\myFigureMacroW{physx_workflow}{Diagram of PhysX SDK workflow}{0.7}

My project uses two PhysX features - rigid body and water simulations and thus those will be described in more details. 

% --------------------------------- Rigid bodies 
\subsection{Rigid bodies}

Basic entity of simulation is actor. Actors can be either static objects, fixed in space (walls, ground, etc.) or dynamic rigid bodies.
Figure~\ref{physx_actors} shows different ways the same actors can be represented. Actors have shapes assigned to them which are used by collision detection system. This shape can be the same mesh as used by rendering system however it is much more efficient to approximate it by set of bounding volumes (figure~\ref{physx_actors}). Bounding volumes are simple shapes for which collision detection is simple and fast - usually rectangles and spheres. 

For physics computation, actor is represented as a tensor located at center of mass (figure~\ref{physx_actors}). In case of collision with other object contact points are provided as points in space with normal vector of collision planes (red arrows on figure~\ref{physx_actors}). Center of mass is automatically computed from actor's shape however there is a possibility to setting it manually.
\figuremacroW{physx_actors}{Different ways of representing actors}{From left: graphical representation for rendering engine, bounding volumes used by collision detection and finally representation used for dynamics.}{1.0}

Important property of actor is its frame. It defines actor local coordinates system. Actor's frame is given as a matrix which transforms points from actor's local coordinates into world coordinates. All actor's shapes positions and orientations are given relatively to actor's frame.

Body movement can be a combination of rectilinear (along path) and angular (around center of mass) motions. Dynamic actors have set of properties used for rigid body simulation. All properties have a linear and angular version. They are gathered in table~\ref{tab:rigid_body_properties}

\begin{table}[htdp]
\caption[Rigid body parameters]{\textbf{Comparision of sphere rendering methods}}
\centering
    \begin{tabular}{|p{6.5cm}|p{7cm}|}
        \hline
        {\bf Linear property } & {\bf Angular property } \\ \hline
       {\bf  mass }          & {\bf  interia } - mass distribution of the body along its three dimensions         \\ 
        {\bf  position } - center of mass position of rigid body       & {\bf  orientation } - 3x3 matrix representing  principal moments relative to the actor's frame     \\ 
        {\bf  velocity } - linear velocity vector      & {\bf  angular velocity } - vector representing axis of rotation which length is equal to magnitude of angular velocity \\ 
        {\bf  force } - vector representing force acting on body     & {\bf  torque } - angular force acting on a body represented as axis of rotation with length equal to magnitude of angular velocity          \\
        \hline
    \end{tabular}
\label{tab:rigid_body_properties}
\end{table}

Physical properties presented in table~\ref{tab:rigid_body_properties} controls how body reacts when force or torque is applied. Actors behavior after collision is determined by theirs material properties. Those parameters are: restitution, static friction and dynamic friction. Normally material is applied to entire actor's shape, however there is a possibility to apply materials per triangle. Additionally friction parameters can be anisotropic. 
% --------------------------------- Fluids
\subsection{Fluids}
PhysX provides three methods for simulating fluids: SPH, simple method and mixed method. First one was described in chapter~\ref{chap:sph}. Second one does not take interparticle forces into concideration thus simplifies computations. This simplified method is good for simulating phenomenas such as smoke or fog, but for realistic water SPH must be used. Third one - mixed method, according to documentation \cite{PhysXDoc}, alternates between SPH and simple mode, providing more performance than SPH but still maintaining some dense characteristics.

Implementation details of PhysX's SPH method are covered in \cite{Harris08}. It solves conservation of momentum equation~\ref{eq:momentum_conservation} to move particles. External force includes only gravity force. Symmetrization of pressure and viscosity forces are done as in equations \ref{eq:sph_pressure_force_muller} and \ref{eq:sph_viscosity_force_sym}. Surface tension (see~\ref{eq:surface_tension}) is also modeled, however there are no implementation details given on that.

There are two ways of creating fluids in PhysX - either directly specifying particles initial positions, velocities, etc. or by using emitters (figure~\ref{emitters_drains}). Emitters are objects that generates particles. They can be static objects or they may be attached to a rigid body and interact with other objects in the scene. Emitters can generate particles in two modes:
\begin{itemize}\itemsep2pt
\item \textit{Constant pressure} - in which state of surrounding fluid is taken into account. Emitter tries to match rest spacing of particles (which is one of fluids parameters). 
\item \textit{Constant flow} - emitter keeps generating the same number of particles each frame. 
\end{itemize}

\figuremacro{emitters_drains}{PhysX emitters and drains}{Simple 2D scene with one emitter and two drains.}

Removing particles can be done in three ways:
\begin{itemize}\itemsep2pt
\item By removing particles manually.
\item By setting particles lifetime parameter which is time in seconds an emitted particle lives.
\item By creating drain objects (figure~\ref{emitters_drains}). When particle hits drain it is removed from simulation. Those object are useful to prevent fluid from spreading too far negatively affecting performance.
\end{itemize}

Fluids can interact with other objects in the scene. The interaction is handled bypassing conservation of momentum equation~\ref{eq:momentum_conservation} by applying velocity changes directly to particles. Interaction with rigid bodies can be either one-way (rigid body acts on fluid) or two-way (when fluid also acts on rigid body).

Most important fluid parameters that can be set are:
\begin{description}\itemsep1pt
\item[Kernel Radius Multiplier] Corresponds to $k$ parameter of SPH smoothing kernel. Together with \textit{Rest Particle Per Meter} defines particles radius of influence (area within smoothing kernel is non-zero): $radius = \frac{kernelRadiusMultiplier}{restParticlesPerMeter}$
\item[Rest Particles Per Meter] Defines number of particles per volume unit when fluid is in rest state.
\item[Rest Density] Corresponds to $\rho_0$ from equation~\ref{eq:pressure}. Defines density of fluid in it's rest state.
\item[Viscosity] Corresponds to $\mu$ constant in equation~\ref{eq:momentum_conservation}. Lower viscosity will produce more runny fluids like water, higher viscosity will produce more sticky fluids like honey.
\item[Stiffness] Corresponds to $k$ constant in equation~\ref{eq:pressure}. Defines how compressible the fluid is. Higher values means less compressible fluids (for instance water). 
\item[Surface Tension] Equivalent of $\sigma$ from equation~\ref{eq:surface_tension}. The higher this parameter is the smoother fluid surface will be. 
\end{description}

PhysX allows creating different fluids with different parameters and simulating them in the same time. However those fluids can't interact. That means it is not possible to simulate mixing two fluids together. Another limitation of PhysX's fluids is limit of 64000 particles per fluid.
% ----------------------------------------------------------------------



